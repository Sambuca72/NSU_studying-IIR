{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63efdb28-ec33-4124-8bdc-427235127d21",
   "metadata": {},
   "source": [
    "# Обучение нейронных сетей"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcb5852-e5aa-40c5-8422-a885707b52ea",
   "metadata": {},
   "source": [
    " ### Один нейрон"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f42c04d-0428-44f5-8994-73a653511f89",
   "metadata": {},
   "source": [
    "Напомним, что нейрон с $n$ входами характеризуется $n+1$ параметром: вектором нормали к разделяющей гиперплоскости $ω={ω_1,...,ω_n}$ и её сдвигом $ω_0$. Расстояние $d$ от вектора входа $x$ к гиперплоскости равно: $$ d=ω_0+ω_1x_1+...+ω_nx_n=ω_0+ωx=\\sum_{i=0}^nω_ix_i. $$ \n",
    "$$x_0 = 1 $$\n",
    "\n",
    "На выходе нейрона $y$ расстояние $d$ нормируется на диапазон [0...1] (прогоняется через активационную функцию \"S\"): $$ y = y(ω,x) = S(d) = S(\\sum_{i=0}^nω_ix_i)$$ $$S(d)={1 \\over 1+e^{−d}}$$\n",
    "\n",
    "Производную сигмоидной функции можно выразить через значение выхода нейрона: $$ S'(d) = {e^{−d} \\over (1 + e^{-d})^2} = S(d)(1−S(d)) =y(1−y). $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e3ab67-2f8f-4c0b-8c59-fa958b0592c5",
   "metadata": {},
   "source": [
    "Пусть на вход нейрона подаётся вектор **x**, в результате чего на выходе получется значение **y**. И пусть на самом деле значению **x** должен соответствовать выход **Y**. Определим ошибку \"работы нейрона\", как среднеквадратичное отклонение желаемого выхода от фактического: $$ E^2=(Y−y(ω,x))^2.$$ **Задача обучения нейрона состоит в минимизации этой ошибки.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9481f92-8516-4314-b2cc-8e0199c66a48",
   "metadata": {},
   "source": [
    "### Градиентный спуск"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0358a8-a8cb-42f1-87ed-314cd597f4ac",
   "metadata": {},
   "source": [
    "Вычислим градиент ошибки. Он направлен в сторону её максимального роста (в обратном направлении ошибка убывает):\n",
    "$${∂E^2 \\over ∂ω_i} = {∂E^2 \\over ∂y}⋅{∂y \\over ∂d}⋅{∂d \\over∂ω_i} = −2(Y−y)⋅y(1−y)⋅x_i$$\n",
    "\n",
    "где учтена производная сигмоидной функции (1). Таким образом, вектор нормали $ω$ и смещение необходимо сдвинуть против градиента на величины: \n",
    "$$ \\delta ω = 2 \\lambda (Y−y)y(1−y)x$$\n",
    "$$ \\delta ω_0 = 2  \\lambda (Y−y)y(1−y) - смещение$$\n",
    "\n",
    "где абсолютную величину сдвига (шаг) задаёт параметр $\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3820eaea-e163-476f-85ff-407a869a938c",
   "metadata": {},
   "source": [
    "Геометрический смысл этих формул прост. Пусть $Y=1$, а нейрон выдаёт $y≪1$. Тогда вектор нормали (и плоскость) повернётся в сторону примера $x$, а $ω_0$ увеличится, т.е. плоскость сдвинется в противоположную от входа сторону. Когда $Y=0$, а $y∼1$, множитель Y−y становится отрицательным и всё произойдёт с точностью до наоборот."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51617161-1e46-4228-8d71-2c297107ae7f",
   "metadata": {},
   "source": [
    "<img src=\"images/LessonsII/grad_step.png\" width=15% height=15% />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e266a917-45e8-4f04-95ee-37226bfce5e5",
   "metadata": {},
   "source": [
    "### Многомерный случай"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d29ba5-0789-4ed5-a0ac-a92093fc9387",
   "metadata": {},
   "source": [
    "Обозначим набор параметров модели вектором $w$. Ошибка является их функцией $L=L(w)$. В пространстве параметров существуют поверхности постоянного значения $L(w)=const$. Смещение $dw$ вдоль таких поверхностей не меняет $L$ и, следовательно, её диффернциал равен нулю:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a2b667-9975-43bf-94b3-637484447f6d",
   "metadata": {},
   "source": [
    "<img src=\"images/LessonsII/dl_dw.png\" width=75% height=75% />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c46e89-dadc-498b-a20b-50b61b6df146",
   "metadata": {},
   "source": [
    "Такими образом, вектор градиента $g$ перпендикулярен поверхностям $L=const$ и направлен в сторону увеличения $L$ (как и любая производная). При приближении к минимуму длина градиента, обычно уменьшается, а в точке минимума (экстремум) она равна нулю (выше второй рисунок).\n",
    "\n",
    "<img src=\"images/LessonsII/g_dl_dw.png\" width=15% height=15% />\n",
    "\n",
    "Чтобы найти минимум $L$, необходимо двигаться в обратном к градиенту направлении (вдоль антиградиента), с шагом пропорциональным некоторому числу $\\lambda$. \n",
    "\n",
    "Этот гиперпараметр называется скоростью обучения:\n",
    "$$ w^{t+1} = w^t − λ g^t $$\n",
    "$$ g^t = {∂L \\over∂w^t}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02786904-7740-4010-b87b-939728bcb145",
   "metadata": {},
   "source": [
    "Чем больше скорость обучения λ, тем быстрее параметры модели приближаются к оптимальным значениям. Однако при больших λ существует риск проскочить минимум (несмотря на уменьшение длины градиента в его окрестности)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3de79d-eb5e-41bb-bf61-0e51220cd331",
   "metadata": {},
   "source": [
    "## Метод Обратного Распространения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356eb766-2474-470c-ab5b-09efe7cf9049",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f46a7f2c-9ba2-4684-9e5f-9518b8e912d5",
   "metadata": {},
   "source": [
    "#### https://habr.com/ru/articles/313216/\n",
    "\n",
    "<img src=\"https://habrastorage.org/files/dad/168/f54/dad168f54a2d4cf0b6508200eda50eef.png\" alt=\"Example\" height=30% width=30%>\n",
    "Ошибка выходного нейрона\n",
    "$$\n",
    "\\delta_{output} = (out_{pred} - out_{y})f'(in)\n",
    "$$\n",
    "Ошибка внутренного нейрона\n",
    "$$\n",
    "\\delta_{hid} = f'(in)\\sum_i(w_i\\delta_i)\n",
    "$$\n",
    "Градиент ошибки нейрона\n",
    "$$\n",
    "grad_a^b = out_a*\\delta_b\n",
    "$$\n",
    "Изменение весов\n",
    "$$\n",
    "\\Delta w_t = E*grad_a^b+\\alpha*\\Delta w_{t-1}\n",
    "$$\n",
    "\n",
    "*E* - скорость обучения  \n",
    "$\\alpha$ - момент"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1731cd0-a80c-433a-9544-bffbaff1f946",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"images/LessonsII/backprop.gif\" width=70% height=70% />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3540cef-533d-4c93-8be4-1565f236a8f5",
   "metadata": {
    "tags": []
   },
   "source": [
    "Ошибка выходного нейрона\n",
    "$$\n",
    "\\delta_{6} = (out_{pred} - out_{y})f'(in)\n",
    "$$\n",
    "\n",
    "Производная от функции активации\n",
    "$$\n",
    "\\sigma(in)' = \\sigma(in) * (1 - \\sigma(in))\n",
    "$$\n",
    "\n",
    "Ошибка внутренного нейрона No5 ($\\delta_{5}$)\n",
    "$$\n",
    "\\delta_{5} = \\sigma(in)' * w_{56} * \\delta_{6} = \\sigma(in)(1 - \\sigma(in)) * w_{56} * \\delta_{6} = out_{y} (1 - out_{y}) * w_{56} * \\delta_{6}\n",
    "$$\n",
    "\n",
    "Градиент ошибки нейрона No5 \n",
    "$$\n",
    "grad_6^5 = \\delta_{5} * f_{5}(in)\n",
    "$$\n",
    "\n",
    "Изменение весов\n",
    "$$\n",
    "w_{56} = w_{56} - \\lambda *grad_5^6\n",
    "$$\n",
    "\n",
    "$\\lambda$ - скорость обучения  \n",
    "$\\alpha$ - момент\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c43c6d2-a4c5-46aa-ac77-6ff2bd37cfbb",
   "metadata": {},
   "source": [
    "### Значение момента\n",
    "![Momentum](https://habrastorage.org/files/4d9/684/061/4d96840618dc44768e57e892033a119a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7c9381-83c9-4a0f-a7a3-30db9fcfbda4",
   "metadata": {},
   "source": [
    "# Полносвязная неронная сеть\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3d/Neural_network.svg/1200px-Neural_network.svg.png\" alt=\"FCNeuralNetwork\" width=30% height=30%>\n",
    "Схема простой нейросети. Зелёным цветом обозначены входные нейроны, голубым — скрытые нейроны, жёлтым — выходной нейрон\n",
    "\n",
    "$$\n",
    "\\vec {h_t} = W_h \\vec x\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\vec h = F_h(\\vec{h_t})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\vec {y_t} = W_y \\vec h\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\vec y = F_y(\\vec{y_t})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f9fc9c-0c4c-4cad-ac77-04da45727ac2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "\n",
    "<img src=\"https://neurohive.io/wp-content/uploads/2018/10/obuchenie-neironnyh-setei-glubokoe.gif\" alt=\"FCNeuralNetwork\" width=60% height=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a5f3e8-7dda-4dcd-97c4-22febeb44b26",
   "metadata": {},
   "source": [
    "# Функци активации\n",
    "## Сигмоида\n",
    "<img src=\"images/LessonsII/sigmoid.png\" width=30% height=30% />\n",
    "\n",
    "$\n",
    "F(x) = {1 \\over 1 + e^{-x}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf65e9ba-9f3d-4170-b597-150469627efd",
   "metadata": {},
   "source": [
    "## Гиперболический тангенс\n",
    "<img src=\"images/LessonsII/tanh.png\" width=30% height=30% />\n",
    "\n",
    "$\n",
    "F(x) = {e^{2x}-1 \\over e^{2x}+1}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60acc1e4-9f35-4763-9d95-0c5f078a3792",
   "metadata": {},
   "source": [
    "## ReLU (rectified linear unit)\n",
    "<img src=\"images/LessonsII/ReLU.png\" width=40% height=40% />\n",
    "\n",
    "$\n",
    "F(x) = max(0, x)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cffddb-a964-4573-83b4-5bae0d2a2082",
   "metadata": {},
   "source": [
    "# Softmax\n",
    "\n",
    "Применяется в тех случаях, когда необходимо, чтобы сумма элементов была равно 1, а каждый элемент принадлежал интервалу \\[0; 1\\] (для задач классификации)\n",
    "\n",
    "\n",
    "$\n",
    "F(x) = {e^{x} \\over \\sum_j e^{x_j}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb71594-5b42-428c-9a2a-15d55f9b1808",
   "metadata": {},
   "source": [
    "<img src=\"images/LessonsII/common_act_func.png\" width=70% height=70% />\n",
    " Скорость обучения logistic-ой функции активации низкая с определенного момента (x > 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0267b1-6de3-4ef4-b057-b129fb4e58d5",
   "metadata": {},
   "source": [
    "# Функция потерь\n",
    "### Регрессия\n",
    "**Средняя абсолютная ошибка**\n",
    "$$\n",
    "MAE = {1\\over n}\\sum_{i=1}^n|y_i - pred_i|\n",
    "$$\n",
    "*n - число признаков в выходном векторе*  \n",
    "*pred - предсказанный вектор*\n",
    "  \n",
    "**Средняя квадратичная ошибка**\n",
    "$$\n",
    "MSE = {1\\over n}\\sum_{i=1}^n(y_i - pred_i)^2\n",
    "$$\n",
    "**Средняя абсолютная процентная ошибка**\n",
    "$$\n",
    "MAPE = {100\\% \\over n}\\sum_{i=1}^n{|y_i - pred_i|\\over y_i}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "958f88f6-0c6a-478e-9ca1-7983b26fb20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scikit-learn\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28814f28-de88-47a4-8d38-b4a8d2bc0e9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20000000000000004"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MAE\n",
    "def mean_absolute_error(y, pred):\n",
    "    diff = y - pred # находим разницу между  наблюдаемыми значениями (y) и прогнозируемыми (pred)\n",
    "    abs_diff = np.absolute(diff) # находим абсолютную разность между прогнозами и фактическими наблюдениями.\n",
    "    mean_diff = abs_diff.mean() # находим среднее значение\n",
    "    return mean_diff\n",
    "\n",
    "y = np.array([1.1,2,1.7]) # создаем список актуальных значений\n",
    "pred = np.array([1,1.7,1.5]) # список прогнозируемых значений\n",
    "\n",
    "# mean_absolute_error(y, pred) # sklearn\n",
    "mean_absolute_error(y, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "545f84b5-7cc0-46b6-b16c-72784c554b9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04666666666666667"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MSE\n",
    "def mean_squared_error(y, pred):\n",
    "    diff = y - pred # находим разницу между  наблюдаемыми значениями (y) и прогнозируемыми (pred)\n",
    "    differences_squared = diff ** 2 # возводим в квадрат (чтобы избавиться от отрицательных значений)\n",
    "    mean_diff = differences_squared.mean() # находим среднее значение\n",
    "\n",
    "    return mean_diff\n",
    "\n",
    "y = np.array([1.1,2,1.7]) \n",
    "pred = np.array([1,1.7,1.5]) \n",
    "\n",
    "# mean_squared_error(y, pred) # sklearn\n",
    "mean_squared_error(y, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83bb67c6-df53-4e26-96ed-9c7d55a76809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21602468994692867"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RMSE\n",
    "def root_mean_squared_error(y, pred):\n",
    "    diff = y - pred # находим разницу между  наблюдаемыми значениями (y) и прогнозируемыми (pred)\n",
    "    differences_squared = diff ** 2 # возводим в квадрат\n",
    "    mean_diff = differences_squared.mean() # находим среднее значение\n",
    "    rmse_val = np.sqrt(mean_diff) # извлекаем квадратный корень\n",
    "    return rmse_val\n",
    "\n",
    "y = np.array([1.1,2,1.7])\n",
    "pred = np.array([1,1.7,1.5])\n",
    "\n",
    "# mean_squared_error(y, pred, squared = False) #Если установлено значение False, функция возвращает значение RMSE.\n",
    "root_mean_squared_error(y, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8929d42-ceac-4707-9b02-5670f171a82a",
   "metadata": {},
   "source": [
    "### Классификация\n",
    "**Бинарная кросс энтропия**\n",
    "$$\n",
    "CE = -y_1log(pred_1)-(1-y_1)log(1-pred_1)\n",
    "$$\n",
    "**Категориальная кросс энтропия**\n",
    "$$\n",
    "CE = -\\sum_j^Cy_jlog(pred_j)\n",
    "$$\n",
    "*C - число классов*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19ab3c6d-c537-4c8f-9e94-1d6b3cee6359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21616187468057912"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def binary_cross_entropy(y, pred):\n",
    "    y = np.array([1 if el == \"Cat\" else 0 for el in y]) # [[0 1 1 0]]\n",
    "    pred = np.array([el[0] for el in pred])\n",
    "    CE = -y*np.log(pred) - (1 - y)*np.log((1-pred))\n",
    "    return CE.mean()\n",
    "\n",
    "labels = {\"Cat\": [1, 0], \"Dog\":[0,1]}\n",
    "\n",
    "def encode_label(y):\n",
    "    return np.array([labels[key] for key in y])\n",
    "\n",
    "def cross_entropy(y, pred):\n",
    "    y = encode_label(y) # [[0, 1], [1, 0], [1, 0], [0, 1]]\n",
    "    pred = np.array(pred)\n",
    "    CE = -np.sum(y*np.log(pred), axis=-1)\n",
    "    return CE.mean()\n",
    "\n",
    "y = [\"Dog\", \"Cat\", \"Cat\", \"Dog\"] # список правильных меток классов\n",
    "pred = [[.1, .9], [.9, .1], [.8, .2], [.35, .65]] # [P(dog), P(cat)] # список вероятностей, предсказанных моделью.\n",
    "\n",
    "# (Первый аргумент в вызове функции — это список правильных меток классов для каждого входа. Второй аргумент — это список вероятностей, предсказанных моделью.)\n",
    "# log_loss(y, pred) # sklearn \n",
    "cross_entropy(y, pred) # binary_cross_entropy(y, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c964808d-0470-4518-bf4d-73e3adef2693",
   "metadata": {},
   "source": [
    "<img src=\"images/LessonsII/cross_entr.png\" alt=\"SoftMax\" height=80% width=80%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014e0f2c-fab2-4c61-b83b-f3f3ae6ebd65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "061d3761-18a9-4ff1-bb0c-9fa89d1ea6b2",
   "metadata": {},
   "source": [
    "# Граф вычислений\n",
    "<img src=\"images/LessonsII/graph.png\" alt=\"gr\" height=50% width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8013da17-f469-4726-a2fa-b9dd9e28be2c",
   "metadata": {},
   "source": [
    "# Обратное распространение ошибки. Пример"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d239481-051f-4d3d-a566-7e35284c2bf3",
   "metadata": {},
   "source": [
    "- функция:\n",
    "$$f(x,w) = 1 + e^{w_1x+w_0}$$\n",
    "<img src=\"images/LessonsII/back1_1.png\" alt=\"Back1\" height=40% width=40%>\n",
    "\n",
    "- Начальное состояение весов и входного значения:\n",
    "<img src=\"images/LessonsII/back1_2.png\" alt=\"back1_2\" height=40% width=40%>\n",
    "\n",
    "- Посчитаем значения по прямому проходу\n",
    "<img src=\"images/LessonsII/back1_3.png\" height=40% width=40%>\n",
    "\n",
    "- Для удобства обозначим узлы (подфункции)\n",
    "<img src=\"images/LessonsII/back1_4.png\" alt=\"back1_4\" height=40% width=40%>\n",
    "\n",
    "- Обратное распространение. Пусть df = 1. Тогда \n",
    "$$dc = {df \\over dc}df = (c+1)'_{c} * 1 = 1$$\n",
    "$$db = {dc \\over db}dc = (e^b)'_{b} * 1 = e^3 = 20$$\n",
    "<img src=\"images/LessonsII/back1_5.png\" height=40% width=40%>\n",
    "\n",
    "$$dw_0 = {db \\over dw_0}db = (a+w_0)'_{w_0} * 20 = 20$$\n",
    "$$da = {db \\over da}db = (a+w_0)'_{a} * 20 = 20$$\n",
    "<img src=\"images/LessonsII/back1_6.png\" height=40% width=40%>\n",
    "\n",
    "$$dw_1 = {da \\over dw_1}da = (x*w_1)'_{w_1} * 20 = x*20 = 20$$\n",
    "$$dx = {da \\over dx}da = (x*w_1)'_{x} * 20 = w_1*20 = 40$$\n",
    "<img src=\"images/LessonsII/back1_7.png\" height=40% width=40%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8434172f-1945-4405-9e7c-f03ec4fd77fe",
   "metadata": {},
   "source": [
    "# Обратное распространение ошибки. Пример с матрицами"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85de85d8-66a9-4e93-b1be-b223ebb251ac",
   "metadata": {},
   "source": [
    "$$f={1 \\over 2} ||X*W||^2_2$$\n",
    "<img src=\"images/LessonsII/back2_1.png\" height=40% width=40%>\n",
    "\n",
    "- Начальное состояение весов и входного значения\n",
    "<img src=\"images/LessonsII/back2_2.png\" height=40% width=40%>\n",
    "\n",
    "- Посчитаем значения по прямому проходу\n",
    "<img src=\"images/LessonsII/back2_3.png\" height=40% width=40%>\n",
    "\n",
    "- Для удобства обозначим узлы (подфункции)  \n",
    "<img src=\"images/LessonsII/back2_4.png\" height=40% width=40%>\n",
    "\n",
    "- Обратное распространение. Пусть df = 1. Тогда \n",
    "$$db = {df \\over db}df = ({1 \\over 2}b)'_{b} * 1 = 0.5$$\n",
    "$$da_{ij} = {db \\over da_{ij}}db = (a_{ij}^2)'_{a_{ij}} * 0.5 = a_{ij} $$\n",
    "или $$\\nabla_a f = a$$\n",
    "<img src=\"images/LessonsII/back2_5.png\" height=40% width=40%>\n",
    "\n",
    "$$\\nabla_w f = X^T \\nabla_a f$$\n",
    "\n",
    "<img src=\"images/LessonsII/back2_6.png\" height=40% width=40%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1523577d-090d-4ff8-a54a-185adcc6d7fc",
   "metadata": {},
   "source": [
    "# Переобучение\n",
    "**Недообучение** — нежелательное явление, возникающее при решении задач обучения по прецедентам, когда алгоритм обучения не обеспечивает достаточно малой величины средней ошибки на обучающей выборке. Недообучение возникает при использовании недостаточно сложных моделей.\n",
    "\n",
    "**Переобучение** (overtraining, overfitting) — нежелательное явление, возникающее при решении задач обучения по прецедентам, когда вероятность ошибки обученного алгоритма на объектах тестовой выборки оказывается существенно выше, чем средняя ошибка на обучающей выборке. Переобучение возникает при использовании избыточно сложных моделей.\n",
    "img src=\"images/LessonsII/Overfitting.svg.png\" width=80% height=80% />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2725a7a-7415-4f55-ad35-732d5b262e7a",
   "metadata": {},
   "source": [
    "### Процесс обучения\n",
    "<img src=\"images/LessonsII/Overfitting.curve.png\" width=30% height=30% />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68158d0-7b85-47c5-a8e2-1de41feb2a29",
   "metadata": {},
   "source": [
    "### Возможные решения при переобучении:\n",
    "* Увеличение количества данных в наборе;\n",
    "* Уменьшение количества параметров модели (количество параметров модели (весов) была в 2 - 3 раза меньше числа примеров обучающего множества);\n",
    "* Добавление регуляризации / увеличение коэффициента регуляризации.\n",
    "\n",
    "### Возможные решения при недообучении:\n",
    "* Добавление новых параметров модели;\n",
    "* Использование для описания модели функций с более высокой степенью;\n",
    "* Уменьшение коэффициента регуляризации."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cf598e-04d7-4e29-a68c-e64d0984fcba",
   "metadata": {},
   "source": [
    "# Аугментация данных\n",
    "\n",
    "**Аугментация данных** (data augmentation) – это методика создания дополнительных обучающих данных из имеющихся данных. Для достижения хороших результатов глубокие сети должны обучаться на очень большом объеме данных. Следовательно, если исходный обучающий набор содержит ограниченное количество изображений, необходимо выполнить аугментацию, чтобы улучшить результаты модели.\n",
    "\n",
    "Можно использовать следующие искажения:\n",
    "* Геометрические (афинные, проективные, ...);\n",
    "* Яркостные/цветовые;\n",
    "* Замена фона;\n",
    "* Искажения, характерные для решаемой задачи: блики, шумы, размытие и т. д."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62648783-ab0d-48ec-91d5-be2662fe6aef",
   "metadata": {},
   "source": [
    "# Обучение с подкреплением\n",
    "Обучение с подкреплением — это метод машинного обучения, при котором происходит обучение модели, которая не имеет сведений о системе, но имеет возможность производить какие-либо действия в ней. Действия переводят систему в новое состояние и модель получает от системы некоторое вознаграждение\n",
    "\n",
    "<img src=\"images/LessonsII/reinforcement-learning-fig1-700.jpg\" alt=\"RF\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b1aec9-c156-4183-872b-ee8d4616e633",
   "metadata": {},
   "source": [
    "## Обучение модели\n",
    "https://media.giphy.com/media/PH67wPdphHPk4/giphy.gif"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11aedb2-b139-419d-abcf-cb8433707f66",
   "metadata": {},
   "source": [
    "# Ресурсы для углублённого изучения темы\n",
    "* Видеолекции Семёна Козлова - (https://www.youtube.com/channel/UCQj_dwbIydi588xrfjWSL5g/featured)\n",
    "* Курс по машинному обучению - (https://www.coursera.org/learn/machine-learning)\n",
    "* Цикл статей об истории нейронных сетей - (http://www.andreykurenkov.com/writing/ai/a-brief-history-of-neural-nets-and-deep-learning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e84311c-2502-42dc-9f11-9e17170356eb",
   "metadata": {},
   "source": [
    "# Задания\n",
    "\n",
    "___\n",
    "\n",
    "# Лабораторная работа 12. Матчасть DL\n",
    "Задача: реализовать и обучить нейронную сеть, состоящую из 3 нейронов (2 слоя), предсказывать значения функции XOR. При выполнении лабораторной запрещается использовать фреймворки для глубокого обучения (как PyTorch, Tensorflow, Caffe, Theano и им подобные).\n",
    "\n",
    "В первую очередь ознакомиться с этим материалом (https://towardsdatascience.com/implementing-the-xor-gate-using-backpropagation-in-neural-networks-c1f255b4f20d).\n",
    "\n",
    "Что необходимо реализовать, используя знания и фрагменты кода из ссылки выше:  \n",
    "1. Класс Neuron, имеющий вектор весов self._weigths\n",
    "2. Два метода класса Neuron: forward(x), backward(x, loss) - реализующих прямой и обратный проход по нейронной сети. \n",
    "   Метод forward должен реализовывать логику работу нейрона: умножение входа на вес self._weigths, сложение и функцию активации сигмоиду. \n",
    "   Метод backward должен реализовывать взятие производной от сигмоиды и используя состояние нейрона обновить его веса.\n",
    "3. Реализовать с помощью класса Neuron нейронную сеть с архитектурой из трёх нейронов, предложенную в статье:\n",
    "<img src=\"images/LessonsII/Neuron.png\" alt=\"Neuron\" height=40% width=40%>\n",
    "\n",
    "\n",
    "4. Для красоты обернуть в класс Model с методами forward и backward, реализующими правильное взаимодействие нейронов на прямом и обратном проходах.\n",
    "5. Реализовать тренировочный цикл следующего вида:\n",
    "\n",
    "```  python\n",
    "цикл (обучающие данные):\n",
    "\ty = model.forward(x)\n",
    "\terr = loss(y, label)\n",
    "\tmodel.backward(x, err)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fc2357-eb33-4bfd-b9c2-b5f0c3c1116e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# В итоге обучения должны предсказываться значения аналогичные описанным в статье.  \n",
    "{(0,0):0, (0,1):1, (1, 0):1, (1,1):0}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
